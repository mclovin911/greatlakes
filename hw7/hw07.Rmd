---
title: "Homework 7"
author: Xingwen Wei
output:
  html_document
---

\newcommand\prob{\mathbb{P}}
\newcommand\E{\mathbb{E}}
\newcommand\var{\mathrm{Var}}
\newcommand\cov{\mathrm{Cov}}
\newcommand\data[1]{#1^*}

--------





### Homework questions

Please submit your homework report to Canvas as both an Rmarkdown (Rmd) file and an html file produced by it. This week, the grader will not run the Rmd file. Next week, we will learn a way to write a reproducible document including computationally intensive results. 

------------

**<big>Question 7.1</big>. Introduction to the greatlakes cluster.**

The greatlakes cluster is a collection of high-performance Linux machines operated by University of Michigan. Each machine has 36 CPU cores. This facilitates computationally intensive Monte Carlo statistical inference, allowing more thorough investigations than are possible on a laptop. Linux cluster computing is the standard platform for computationally intensive statistics and data science, so experience working with them is worthwhile.

Using greatlakes is optional for your STATS/DATASCI 531 final project. However, you may find that once you have run a simple parallel R command, following the instructions below, it is fairly straightforward to run the code for your project.

Read the [greatlakes notes on the course website](../greatlakes/index.html) and work through the example to run the parallel foreach in the file [test.R](../greatlakes/test.R) on greatlakes. If you are already familiar with greatlakes, Question 7.1 may be trivial, otherwise it is a good experience.


Have you used a Linux cluster before? Report briefly on whether you successfully ran the test code. Mention any issues that you had to overcome. 

**Answer 7.1**

I have used remote Linux server before. I have successfully ran the test code on greatlakes. There was no difficulties.

----------

**<big>Question 7.2</big>. Investigating the SEIR model.**

We consider an SEIR model for the Consett measles epidemic, which is the same model and data used for Homework 6. Write a report presenting the following steps. You will need to tailor the intensity of your search to the computational resources at your disposal. In particular, choose the number of starting points, number of particles employed, and the number of IF2 iterations appropriately for the size and speed of your machine. It is okay for this homework if the Monte Carlo error is larger than you would like. Optionally, you can run this on greatlakes. Whether you run it on greatlakes or a laptop or some other machine, your code should take advantage of multiple processors.

(a) Conduct a local search and then a global search using the multi-stage, multi-start method.

(b) How does the maximized likelihood for the SEIR model compare with what we obtained for the SIR model?

(c) How do the parameter estimates differ?

(d) Calculate and plot a profile likelihood over the reporting rate for the SEIR model. Construct a 95% confidence interval for the reporting rate, and discuss how this profile compares with the SIR profile in Chapter 14.

**Answer 7.2**

As before, we download the data and implement pomp in C snippets.

a) First we do a local search start with the initial parameter values, Beta=40, mu_IR=1.3, mu_EI=0.8, rho=0.5, eta=0.06, N=38000 where we fix the values of mu_IR and N as truth.
We do particle filters 30 times to get an unbiased likelihood estimate of -271 with a Monte Carlo standard error of 5.4.
Although we would like a smaller standard error, the additional computational cost is above the capability of this machine.

```{r, echo=FALSE, message=FALSE}
library(foreach)
library(doParallel)
registerDoParallel()

library(doRNG)
registerDoParallel()
registerDoRNG(2488820)
library(tidyverse)
library(pomp)

read_csv(paste0("https://kingaa.github.io/sbied/stochsim/",
                "Measles_Consett_1948.csv")) %>%
    select(week,reports=cases) -> meas

sir_step <- Csnippet("
  double dN_SE = rbinom(S,1-exp(-Beta*I/N*dt));
  double dN_EI = rbinom(E,1-exp(-mu_EI*dt));
  double dN_IR = rbinom(I,1-exp(-mu_IR*dt));
  S -= dN_SE;
  E += dN_SE - dN_EI;
  I += dN_EI - dN_IR;
  H += dN_IR;
  ")

sir_rinit <- Csnippet("
  S = nearbyint(eta*N);
  E = 0;
  I = 1;
  H = 0;
  ")

sir_dmeas <- Csnippet("
  lik = dbinom(reports,H,rho,give_log);
  ")

sir_rmeas <- Csnippet("
  reports = rbinom(H,rho);
  ")

meas %>%
  pomp(times="week",t0=0,
       rprocess=euler(sir_step,delta.t=1/7),
       rinit=sir_rinit,
       rmeasure=sir_rmeas,
       dmeasure=sir_dmeas,
       accumvars="H",
       partrans=parameter_trans(log=c('Beta', 'mu_EI'), logit=c('rho', 'eta')),
       statenames=c("S","E", "I","H"),
       paramnames=c("Beta","mu_IR","mu_EI","N","eta","rho")
       ) -> measSIR

params <- c(Beta=40, mu_IR=1.3, mu_EI=0.8, rho=0.5, eta=0.06, N=38000)

fixed_params <- c(N=38000, mu_IR=2)
```

```{r}
registerDoRNG(123294940)
foreach(i=1:30,.combine=c) %dopar% {
  library(pomp)
  measSIR %>% pfilter(params=params,Np=50000)
} -> pf

pf %>% logLik() %>% logmeanexp(se=TRUE) -> L_pf
L_pf
```
```{r, echo=FALSE}
pf[[1]] %>% coef() %>% bind_rows() %>%
  bind_cols(loglik=L_pf[1],loglik.se=L_pf[2]) %>%
  write_csv("measles_params.csv")
```


Here we do a local search via iterative filters around the initial guess in the parameter space.
We choose a common perturbation size of 0.02 across all the parameters and cooling.fraction.50=0.5 for mif2 to reduce perturbation size in half after 50 iterations.
We see the likelihood increases as iteration goes, so the iterative filter is working.
We find higher likelihood is usually associated with lower values of rho and higher values of mu_EI.
There is no clear trend for Beta and eta.
N and mu_IR are a flat line because we fixed their value with no perturbation.

```{r}
registerDoRNG(482947940)
bake(file="local_search.rds",{
  foreach(i=1:20,.combine=c) %dopar% {
    library(pomp)
    library(tidyverse)
    measSIR %>%
      mif2(
        params=params,
        Np=2000, Nmif=50,
        cooling.fraction.50=0.5,
        rw.sd=rw.sd(Beta=0.02, rho=0.02, mu_EI=0.02, eta=ivp(0.02))
      )
  } -> mifs_local
  mifs_local
}) -> mifs_local

mifs_local %>%
  traces() %>%
  melt() %>%
  ggplot(aes(x=iteration,y=value,group=L1,color=factor(L1)))+
  geom_line()+
  guides(color=FALSE)+
  facet_wrap(~variable,scales="free_y")
```

Because of the perturbations applied to iterative filter, we use particle filter again to evaluate their likelihood for each point estimate.
According to the pairwise scatter plot below, there might be a ridge in the likelihood surface of {Beta and mu_EI} and {Beta and eta}.

```{r}
registerDoRNG(900242057)
bake(file="lik_local.rds",{
  foreach(mf=mifs_local,.combine=rbind) %dopar% {
    library(pomp)
    library(tidyverse)
    evals <- replicate(10, logLik(pfilter(mf,Np=20000)))
    ll <- logmeanexp(evals,se=TRUE)
    mf %>% coef() %>% bind_rows() %>%
      bind_cols(loglik=ll[1],loglik.se=ll[2])
  } -> results
  results
}) -> results

pairs(~loglik+Beta+mu_EI+eta+rho,data=results,pch=16)

```

```{r, echo=FALSE, message=FALSE}
read_csv("measles_params.csv") %>%
  bind_rows(results) %>%
  arrange(-loglik) %>%
  write_csv("measles_params.csv")

```


Then, we want to do a global search of the likelihood surface.
We use a box that contains reasonable parameter values.
$\beta \in (5, 80), \mu_{EI} \in (0, 5), \rho\in(0.2, 0.9), \eta\in(0, 0.4).$
We start with points uniformly selected from the possible box.
We re-run iterative filter from the endpoints from before.
After all the iterative filters for all the starting points, we use particle filter to evaluate the likelihood.
The best result of this global search has likelihood -120 with a standard error of 0.65. 
For example, we can see Beta converges to 10 to 30, mu_EI converges to 10 to 20.
To get a closer look, we can look at each parameter's profile likelihood.

```{r}
set.seed(2062379496)

runif_design(
  lower=c(Beta=5,mu_EI=0, rho=0.2,eta=0),
  upper=c(Beta=80,mu_EI=5, rho=0.9,eta=0.4),
  nseq=100
) -> guesses

mf1 <- mifs_local[[1]]

bake(file="global_search.rds",{
  registerDoRNG(1270401374)
  foreach(guess=iter(guesses,"row"), .combine=rbind) %dopar% {
    library(pomp)
    library(tidyverse)
    mf1 %>%
      mif2(params=c(unlist(guess),fixed_params)) %>%
      mif2(Nmif=50) -> mf
    replicate(
      10,
      mf %>% pfilter(Np=1000) %>% logLik()
    ) %>%
      logmeanexp(se=TRUE) -> ll
    mf %>% coef() %>% bind_rows() %>%
      bind_cols(loglik=ll[1],loglik.se=ll[2])
  } -> results
  results
}) %>%
  filter(is.finite(loglik)) -> results


```

```{r, echo=FALSE, messgae=FALSE}
read_csv("measles_params.csv") %>%
  bind_rows(results) %>%
  filter(is.finite(loglik)) %>%
  arrange(-loglik) %>%
  write_csv("measles_params.csv")

read_csv("measles_params.csv") %>%
  filter(loglik>max(loglik)-50) %>%
  bind_rows(guesses) %>%
  mutate(type=if_else(is.na(loglik),"guess","result")) %>%
  arrange(type) -> all



```
```{r, echo=FALSE}
pairs(~loglik+Beta+mu_EI+eta+rho, data=all,
      col=ifelse(all$type=="guess",grey(0.5),"red"),pch=16)
```

```{r}
read_csv("measles_params.csv") %>%
  filter(loglik>max(loglik)-20,loglik.se<2) %>%
  sapply(range) -> box

set.seed(1196696958)
profile_design(
  eta=seq(0.01,0.85,length=10),
  lower=box[1,c("Beta","mu_EI","rho")],
  upper=box[2,c("Beta","mu_EI","rho")],
  nprof=10, type="runif"
) -> guesses

mf1 <- mifs_local[[1]]
```

```{r}
registerDoRNG(830007657)
bake(file="eta_profile.rds",{
  foreach(guess=iter(guesses,"row"), .combine=rbind) %dopar% {
    library(pomp)
    library(tidyverse)
    mf1 %>%
      mif2(params=c(unlist(guess),fixed_params),
           rw.sd=rw.sd(Beta=0.02,mu_EI=0.02, rho=0.02)) %>%
      mif2(Nmif=100,cooling.fraction.50=0.3) -> mf
    replicate(
      10,
      mf %>% pfilter(Np=100) %>% logLik()) %>%
      logmeanexp(se=TRUE) -> ll
    mf %>% coef() %>% bind_rows() %>%
      bind_cols(loglik=ll[1],loglik.se=ll[2])
  } -> results
  results
}) -> results

```


```{r}
read_csv("measles_params.csv") %>%
  bind_rows(results) %>%
  filter(is.finite(loglik)) %>%
  arrange(-loglik) %>%
  write_csv("measles_params.csv")

read_csv("measles_params.csv") %>%
  filter(loglik>max(loglik)-10) -> all

pairs(~loglik+Beta+mu_EI+eta+rho,data=all,pch=16)
```

```{r}
maxloglik <- max(results$loglik,na.rm=TRUE)
ci.cutoff <- maxloglik-0.5*qchisq(df=1,p=0.95)

results %>%
  filter(is.finite(loglik)) %>%
  group_by(round(eta,5)) %>%
  filter(rank(-loglik)<3) %>%
  ungroup() %>%
  ggplot(aes(x=eta,y=loglik))+
  geom_point()+
  geom_smooth(method="loess",span=0.25)+
  geom_hline(color="red",yintercept=ci.cutoff)+
  lims(y=maxloglik-c(5,0))
```

```{r}
read_csv("measles_params.csv") %>%
  group_by(cut=round(rho,2)) %>%
  filter(rank(-loglik)<=10) %>%
  ungroup() %>%
  select(-cut,-loglik,-loglik.se) -> guesses

mf1 <- mifs_local[[1]]
registerDoRNG(2105684752)
bake(file="rho_profile.rds",{
  foreach(guess=iter(guesses,"row"), .combine=rbind) %dopar% {
    library(pomp)
    library(tidyverse)
    mf1 %>%
      mif2(params=guess,
           rw.sd=rw.sd(Beta=0.02,mu_EI=0.02, eta=ivp(0.02))) %>%
      mif2(Nmif=100,cooling.fraction.50=0.3) %>%
      mif2() -> mf
    replicate(
      10,
      mf %>% pfilter(Np=10000) %>% logLik()) %>%
      logmeanexp(se=TRUE) -> ll
    mf %>% coef() %>% bind_rows() %>%
      bind_cols(loglik=ll[1],loglik.se=ll[2])
  } -> results
  results
}) -> results

read_csv("measles_params.csv") %>%
  bind_rows(results) %>%
  filter(is.finite(loglik)) %>%
  arrange(-loglik) %>%
  write_csv("measles_params.csv")

results %>%
  filter(is.finite(loglik)) -> results

pairs(~loglik+Beta+mu_EI+eta+rho,data=results,pch=16)
```

```{r}
results %>%
  filter(loglik>max(loglik)-10,loglik.se<1) %>%
  group_by(round(rho,2)) %>%
  filter(rank(-loglik)<3) %>%
  ungroup() %>%
  ggplot(aes(x=rho,y=loglik))+
  geom_point()+
  geom_hline(
    color="red",
    yintercept=max(results$loglik)-0.5*qchisq(df=1,p=0.95)
  )


```
According to the results, we achieve the highest likelihood = -120.6 with the set of parameter values $\beta=28.8, \mu_{IR}=1.3, \mu_{EI}=20.8, \rho=0.42, \eta=0.06, N=38000$. 

----------

**<big>Question 7.3</big>. This feedback response is worth credit.**

(a) Explain which parts of your responses above made use of a source, meaning anything or anyone you consulted (including your class group, or other classmates, or online solutions to previous courses) to help you write or check your answers. All sources are permitted, but you are expected to explain clearly what is, and is not, your own original contribution, as discussed in the [syllabus](../syllabus.html).

(b) As for homework 6, this homework is conceptually a routine adaptation of existing code, but involves overcoming various technical hurdles. The hurdles may be overcome quite quickly, or could turn into a longer battle. Once you have finished this homework, you are in a position to carry out data analysis for a wide range of POMP models. How long did this homework take? Report on any technical difficulties that arose.


---------------

### Acknowledgements

Question 7.2 derives from material in [Simulation-based Inference for Epidemiological Dynamics](http://kingaa.github.io/sbied/index.html).

---------------




